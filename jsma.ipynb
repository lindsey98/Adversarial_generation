{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')           # noqa: E402\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from attacks import jsma\n",
    "\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "img_chan = 3\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR10\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading CIFAR10')\n",
    "\n",
    "cifar = tf.keras.datasets.cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar.load_data()\n",
    "print(np.shape(X_train))\n",
    "X_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\n",
    "X_train = X_train.astype(np.float32) / 255\n",
    "X_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spliting data\n"
     ]
    }
   ],
   "source": [
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "n = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\n",
    "X_valid = X_train[n:]\n",
    "X_train = X_train[:n]\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construction graph\n",
      "WARNING:tensorflow:From <ipython-input-6-5a9d998410f7>:8: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-5a9d998410f7>:10: max_pooling2d (from tensorflow.python.keras.legacy_tf_layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-5a9d998410f7>:22: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-5a9d998410f7>:23: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /Users/i537199/Desktop/Adversarial_generation/attacks/saliency_map.py:79: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "print('\\nConstruction graph')\n",
    "\n",
    "\n",
    "def model(x, logits=False, training=False):\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('conv0'):\n",
    "        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n",
    "                             padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('flatten'):\n",
    "        shape = z.get_shape().as_list()\n",
    "        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n",
    "\n",
    "    with tf.variable_scope('mlp'):\n",
    "        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n",
    "        z = tf.layers.dropout(z, rate=0.25, training=training)\n",
    "\n",
    "    logits_ = tf.layers.dense(z, units=10, name='logits')\n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "\n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n",
    "\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "env = Dummy()\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n",
    "                           name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder_with_default(False, (), name='mode')\n",
    "    \n",
    "    env.ybar, logits = model(env.x, logits=True, training=env.training)\n",
    "\n",
    "    with tf.variable_scope('acc'):\n",
    "        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n",
    "        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
    "                                                       logits=logits)\n",
    "        env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        env.train_op = optimizer.minimize(env.loss)\n",
    "\n",
    "    env.saver = tf.train.Saver()\n",
    "\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.target = tf.placeholder(tf.int32, (), name='target')\n",
    "    env.adv_epochs = tf.placeholder_with_default(20, shape=(), name='epochs')\n",
    "    env.adv_eps = tf.placeholder_with_default(0.2, shape=(), name='eps')\n",
    "    env.x_jsma = jsma(model, env.x, env.target, eps=env.adv_eps, k=1,\n",
    "                      epochs=env.adv_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing graph\n"
     ]
    }
   ],
   "source": [
    "print('\\nInitializing graph')\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, env, X_data, y_data, batch_size=128):\n",
    "    \"\"\"\n",
    "    Evaluate TF model by running env.loss and env.acc.\n",
    "    \"\"\"\n",
    "    print('\\nEvaluating')\n",
    "    startTime = time.time()\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    loss, acc = 0, 0\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        cnt = end - start\n",
    "        batch_loss, batch_acc = sess.run(\n",
    "            [env.loss, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end]})\n",
    "        loss += batch_loss * cnt\n",
    "        acc += batch_acc * cnt\n",
    "    loss /= n_sample\n",
    "    acc /= n_sample\n",
    "    endTime = time.time()\n",
    "\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    print(endTime - startTime)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n",
    "          load=False, shuffle=True, batch_size=128, name='model'):\n",
    "    \"\"\"\n",
    "    Train a TF model by running env.train_op.\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        if not hasattr(env, 'saver'):\n",
    "            return print('\\nError: cannot find saver op')\n",
    "        print('\\nLoading saved model')\n",
    "        return env.saver.restore(sess, 'model/{}'.format(name))\n",
    "\n",
    "    print('\\nTrain model')\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "        if shuffle:\n",
    "            print('\\nShuffling data')\n",
    "            ind = np.arange(n_sample)\n",
    "            np.random.shuffle(ind)\n",
    "            X_data = X_data[ind]\n",
    "            y_data = y_data[ind]\n",
    "\n",
    "        for batch in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "            start = batch * batch_size\n",
    "            end = min(n_sample, start + batch_size)\n",
    "            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n",
    "                                              env.y: y_data[start:end],\n",
    "                                              env.training: True})\n",
    "        if X_valid is not None:\n",
    "            evaluate(sess, env, X_valid, y_valid)\n",
    "\n",
    "    if hasattr(env, 'saver'):\n",
    "        print('\\n Saving model')\n",
    "        os.makedirs('model', exist_ok=True)\n",
    "        env.saver.save(sess, 'model/{}'.format(name))\n",
    "\n",
    "\n",
    "def predict(sess, env, X_data, batch_size=128):\n",
    "    \"\"\"\n",
    "    Do inference by running env.ybar.\n",
    "    \"\"\"\n",
    "    print('\\nPredicting')\n",
    "    n_classes = env.ybar.get_shape().as_list()[1]\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    yval = np.empty((n_sample, n_classes))\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        y_batch = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end],\n",
    "            env.target: np.random.choice(n_classes)})\n",
    "        print(y_batch)\n",
    "        yval[start:end] = y_batch\n",
    "    print()\n",
    "    return yval\n",
    "\n",
    "\n",
    "# To do the boundary analysis for adversarial samples\n",
    "def boundary_analysis(sess, env, X_data, y_data, batch_size=128):\n",
    "    \"\"\"\n",
    "    Evaluate TF model by running env.loss and env.acc.\n",
    "    \"\"\"\n",
    "    print('\\nPredicting')\n",
    "    n_classes = env.ybar.get_shape().as_list()[1]\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    yval = np.empty((n_sample, n_classes))\n",
    "\n",
    "    img_count = 0\n",
    "    diff_confidence_score_total = 0.0\n",
    "    for batch in range(n_batch):\n",
    "        #print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        y_batch = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end],\n",
    "            env.target: np.random.choice(n_classes)})\n",
    "        for yy in y_batch:\n",
    "            yy2 = yy.tolist()\n",
    "            yy2.sort(reverse=True)\n",
    "            first_confidence_score = yy2[0]\n",
    "            second_confidence_score = yy2[1]\n",
    "            diff_confidence_score = first_confidence_score - second_confidence_score\n",
    "            diff_confidence_score_total += diff_confidence_score\n",
    "            img_count += 1\n",
    "        yval[start:end] = y_batch\n",
    "    print(diff_confidence_score_total / img_count)\n",
    "    print()\n",
    "    return yval\n",
    "\n",
    "\n",
    "def make_jsma(sess, env, X_data, epochs=0.2, eps=1.0, batch_size=128):\n",
    "    \"\"\"\n",
    "    Generate JSMA by running env.x_jsma.\n",
    "    \"\"\"\n",
    "    print('\\nMaking adversarials via JSMA')\n",
    "\n",
    "    startTime = time.time()\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample + batch_size - 1) / batch_size)\n",
    "    X_adv = np.empty_like(X_data)\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        feed_dict = {\n",
    "            env.x: X_data[start:end],\n",
    "            env.target: np.random.choice(n_classes),\n",
    "            env.adv_epochs: epochs,\n",
    "            env.adv_eps: eps}\n",
    "        adv = sess.run(env.x_jsma, feed_dict=feed_dict)\n",
    "        X_adv[start:end] = adv\n",
    "    endTime = time.time()\n",
    "    print()\n",
    "    print(endTime - startTime)\n",
    "\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "Loading saved model\n",
      "INFO:tensorflow:Restoring parameters from model/cifar10\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey model/train_op/model/conv0/conv2d/bias/Adam not found in checkpoint\n\t [[node model/save/RestoreV2 (defined at <ipython-input-6-5a9d998410f7>:61) ]]\n\nOriginal stack trace for 'model/save/RestoreV2':\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-5a9d998410f7>\", line 61, in <module>\n    env.saver = tf.train.Saver()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 836, in __init__\n    self.build()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 848, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 876, in _build\n    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 515, in _build_internal\n    restore_op = self._AddRestoreOps(filename_tensor, saveables,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 335, in _AddRestoreOps\n    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 583, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1521, in restore_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1440\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1441\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key model/train_op/model/conv0/conv2d/bias/Adam not found in checkpoint\n\t [[{{node model/save/RestoreV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1297\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         sess.run(self.saver_def.restore_op_name,\n\u001b[0m\u001b[1;32m   1299\u001b[0m                  {self.saver_def.filename_tensor_name: save_path})\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    958\u001b[0m                          run_metadata_ptr)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1359\u001b[0m                            run_metadata)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key model/train_op/model/conv0/conv2d/bias/Adam not found in checkpoint\n\t [[node model/save/RestoreV2 (defined at <ipython-input-6-5a9d998410f7>:61) ]]\n\nOriginal stack trace for 'model/save/RestoreV2':\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-5a9d998410f7>\", line 61, in <module>\n    env.saver = tf.train.Saver()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 836, in __init__\n    self.build()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 848, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 876, in _build\n    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 515, in _build_internal\n    restore_op = self._AddRestoreOps(filename_tensor, saveables,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 335, in _AddRestoreOps\n    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 583, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1521, in restore_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     return CheckpointReader.CheckpointReader_GetTensor(\n\u001b[0m\u001b[1;32m     70\u001b[0m         self, compat.as_bytes(tensor_str))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1308\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1626\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m   \u001b[0mobject_graph_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_GRAPH_PROTO_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m   \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableObjectGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8ca23a81cc37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTraining'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(sess, env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n\u001b[0m\u001b[1;32m      4\u001b[0m       name='cifar10')\n",
      "\u001b[0;32m<ipython-input-8-1a64ca7947af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, X_data, y_data, X_valid, y_valid, epochs, load, shuffle, batch_size, name)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nError: cannot find saver op'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nLoading saved model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTrain model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;31m# is a graph mismatch. Re-raise the original error with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         raise _wrap_restore_error_with_msg(\n\u001b[0m\u001b[1;32m   1315\u001b[0m             err, \"a Variable name or other graph key that is missing\")\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey model/train_op/model/conv0/conv2d/bias/Adam not found in checkpoint\n\t [[node model/save/RestoreV2 (defined at <ipython-input-6-5a9d998410f7>:61) ]]\n\nOriginal stack trace for 'model/save/RestoreV2':\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-5a9d998410f7>\", line 61, in <module>\n    env.saver = tf.train.Saver()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 836, in __init__\n    self.build()\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 848, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 876, in _build\n    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 515, in _build_internal\n    restore_op = self._AddRestoreOps(filename_tensor, saveables,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 335, in _AddRestoreOps\n    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 583, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1521, in restore_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"/Users/i537199/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining')\n",
    "\n",
    "train(sess, env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n",
    "      name='cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on clean data\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3052 acc: 0.0997\n",
      "5.4840922355651855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.3051980388641358, 0.0997)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nEvaluating on clean data')\n",
    "\n",
    "evaluate(sess, env, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating adversarial data\n",
      "\n",
      "Making adversarials via JSMA\n",
      " batch 352/352\n",
      "924.8766701221466\n",
      "\n",
      "Making adversarials via JSMA\n",
      " batch 352/352\n",
      "989.5228879451752\n"
     ]
    }
   ],
   "source": [
    "print('\\nGenerating adversarial data')\n",
    "\n",
    "X_adv = make_jsma(sess, env, X_train, epochs=30, eps=0)\n",
    "X_adv1 = make_jsma(sess, env, X_train, epochs=30, eps=0.1)\n",
    "#X_adv2 = make_jsma(sess, env, X_test, epochs=30, eps=0.2)\n",
    "#X_adv3 = make_jsma(sess, env, X_test, epochs=30, eps=0.3)\n",
    "#X_adv4 = make_jsma(sess, env, X_test, epochs=30, eps=0.4)\n",
    "#X_adv5 = make_jsma(sess, env, X_test, epochs=30, eps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for imageArr in X_adv1:\n",
    "    im = Image.fromarray((imageArr * 255).astype(np.uint8))\n",
    "    im.save(\"jsma_perturbed_images/0.1/\" + str(i) + \".png\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on adversarial data\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3188 acc: 0.1062\n",
      "2.4008052349090576\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3190 acc: 0.1044\n",
      "2.2438721656799316\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3192 acc: 0.1055\n",
      "2.197758913040161\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3193 acc: 0.1051\n",
      "2.3111629486083984\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3195 acc: 0.1035\n",
      "2.2774298191070557\n",
      "\n",
      "Evaluating\n",
      " loss: 2.3193 acc: 0.1048\n",
      "2.5419719219207764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.319290768814087, 0.1048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nEvaluating on adversarial data')\n",
    "\n",
    "evaluate(sess, env, X_adv, y_test)\n",
    "evaluate(sess, env, X_adv1, y_test)\n",
    "evaluate(sess, env, X_adv2, y_test)\n",
    "evaluate(sess, env, X_adv3, y_test)\n",
    "evaluate(sess, env, X_adv4, y_test)\n",
    "evaluate(sess, env, X_adv5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDoing boundary analysis on adversarial data')\n",
    "\n",
    "print('\\nDifference in confidence score when eps = 0.0')\n",
    "boundary_analysis(sess, env, X_adv, y_test)\n",
    "print('\\nDifference in confidence score when eps = 0.1')\n",
    "boundary_analysis(sess, env, X_adv1, y_test)\n",
    "print('\\nDifference in confidence score when eps = 0.2')\n",
    "boundary_analysis(sess, env, X_adv2, y_test)\n",
    "print('\\nDifference in confidence score when eps = 0.3')\n",
    "boundary_analysis(sess, env, X_adv3, y_test)\n",
    "print('\\nDifference in confidence score when eps = 0.4')\n",
    "boundary_analysis(sess, env, X_adv4, y_test)\n",
    "print('\\nDifference in confidence score when eps = 0.5')\n",
    "boundary_analysis(sess, env, X_adv5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRandomly sample adversarial data from each category')\n",
    "\n",
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(predict(sess, env, X_test), axis=1)\n",
    "ind = z0 == z1\n",
    "\n",
    "X_data = X_test[ind]\n",
    "labels = z0[ind]\n",
    "\n",
    "X_adv = np.empty((10, 10, 32, 32, 3))\n",
    "\n",
    "for source in np.arange(10):\n",
    "    print('Source label {0}'.format(source))\n",
    "\n",
    "    X_i = X_data[labels == source]\n",
    "\n",
    "    for i, xi in enumerate(X_i):\n",
    "        found = True\n",
    "        xi = xi[np.newaxis, :]\n",
    "\n",
    "        for target in np.arange(10):\n",
    "            print(' [{0}/{1}] {2} -> {3}'\n",
    "                  .format(i+1, X_i.shape[0], source, target), end='')\n",
    "\n",
    "            if source == target:\n",
    "                xadv = xi.copy()\n",
    "            else:\n",
    "                feed_dict = {env.x: xi, env.target: target, env.adv_epochs: 30,\n",
    "                             env.adv_eps: 0.12}\n",
    "                xadv = sess.run(env.x_jsma, feed_dict=feed_dict)\n",
    "\n",
    "            yadv = predict(sess, env, xadv)\n",
    "            label = np.argmax(yadv.flatten())\n",
    "            found = target == label\n",
    "\n",
    "            if not found:\n",
    "                print(' Fail')\n",
    "                break\n",
    "\n",
    "            X_adv[source, target] = np.squeeze(xadv)\n",
    "            print(' res: {0} {1:.2f}'.format(label, np.max(yadv)))\n",
    "\n",
    "        if found:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGenerating figure')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(10, 10, wspace=0.1, hspace=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.imshow(X_adv[i, j], cmap='gray', interpolation='none')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        if i == j:\n",
    "            for spine in ax.spines:\n",
    "                ax.spines[spine].set_color('green')\n",
    "                ax.spines[spine].set_linewidth(5)\n",
    "\n",
    "        if ax.is_first_col():\n",
    "            ax.set_ylabel(i, fontsize=20, rotation='horizontal', ha='right')\n",
    "        if ax.is_last_row():\n",
    "            ax.set_xlabel(j, fontsize=20)\n",
    "\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs('img', exist_ok=True)\n",
    "plt.savefig('img/jsma_cifar10_diff.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
